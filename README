# 1. 実験概要

- **実験名称**: 「AI学習によるディクテーターゲーム実験」
- **目的**:  
  - 15回のディクテーターゲームで得られた人間の選択データを用いて、AI（ランダムフォレスト）に予測させ、その予測結果を実際に報酬に反映させる。
  - 「将来世代への外部性」や「ロール切り替え」等は省略し、シンプルに自分たち（同じペア）の報酬にのみ影響する設計とする。

- **対象**: オンライン実験もしくは実験室型実験で参加する被験者
- **大まかなフロー**:
  1. **Part 1**: 実労働タスク（すべての参加者が同一の初期ポイントを得る）
  2. **Part 2**: ディクテーターゲーム（15回）
     - 同じペアを固定して実施
     - 一方がディクテーター（A役）、もう一方が受け手（B役）
     - 役割固定
  3. **Part 2 追加ラウンド**: 16回目のラウンド
     - AI学習条件：AIによる予測
     - 基本条件：通常のディクテーターゲーム
  4. **アンケート（Part 3）**: デモグラ情報・AIへの認知や満足度等の質問

- **報酬支払い**:
  - 全16ラウンドから「ランダムに2回分」を選択
  - 実労働タスクの完了報酬
  - 以上を合算して最終的に参加者へ支払う

---
ペイオフ設定
| Game | Option_X_Dictator | Option_X_Receiver | Option_Y_Dictator | Option_Y_Receiver | Category_Slope | Category_Position | Category_Efficiency | Category_Inequality | Is_Training |
|------|-------------------|-------------------|-------------------|-------------------|----------------|-------------------|-------------------|-------------------|-------------|
| 2*   | 910              | 140               | 830              | 520               | Selfish        | Advantageous      | Y                 | Y                 | TRUE        |
| 5*   | 1010             | 190               | 730              | 470               | Selfish        | Advantageous      | None              | Y                 | TRUE        |
| 7    | 1060             | 330               | 680              | 330               | Receiver indiff.| Advantageous      | X                 | Y                 | TRUE        |
| 8    | 990              | 480               | 750              | 180               | X Pareto       | Advantageous      | X                 | X                 | TRUE        |
| 10   | 870              | 140               | 870              | 520               | Dictator indiff.| Advantageous      | Y                 | Y                 | TRUE        |
| 12*  | 640              | 410               | 560              | 790               | Selfish        | Mixed             | Y                 | None              | TRUE        |
| 15*  | 740              | 460               | 460              | 740               | Selfish        | Mixed             | None              | None              | TRUE        |
| 17   | 790              | 600               | 410              | 600               | Receiver indiff.| Mixed             | X                 | None              | TRUE        |
| 18   | 720              | 750               | 480              | 450               | X Pareto-dom.  | Mixed             | X                 | None              | TRUE        |
| 20   | 600              | 410               | 600              | 790               | Dictator indiff.| Mixed             | Y                 | None              | TRUE        |
| 22*  | 370              | 680               | 290              | 1060              | Selfish        | Disadvantageous   | Y                 | X                 | TRUE        |
| 25*  | 470              | 730               | 190              | 1010              | Selfish        | Disadvantageous   | None              | X                 | TRUE        |
| 27   | 520              | 870               | 140              | 870               | Receiver indiff.| Disadvantageous   | X                 | X                 | TRUE        |
| 28   | 450              | 1020              | 210              | 720               | X Pareto-dom.  | Disadvantageous   | X                 | Y                 | TRUE        |
| 30   | 330              | 680               | 330              | 1060              | Dictator indiff.| Disadvantageous   | Y                 | X                 | TRUE        |
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
| 1    | 1030             | 220               | 710              | 440               | Selfish        | Advantageous      | X                 | Y                 | FALSE       |
| 2    | 960              | 500               | 780              | 160               | X Pareto       | Advantageous      | X                 | X                 | FALSE       |
| 3    | 760              | 490               | 440              | 710               | Selfish        | Mixed             | X                 | None              | FALSE       |
| 4    | 690              | 770               | 510              | 430               | X Pareto       | Mixed             | X                 | None              | FALSE       |
| 5    | 490              | 760               | 170              | 980               | Selfish        | Disadvantageous   | X                 | X                 | FALSE       |
| 6    | 420              | 1040              | 240              | 700               | X Pareto       | Disadvantageous   | X                 | Y                 | FALSE       |

■ 各列の説明
・Game: オリジナルの表からのゲーム番号
・Option X (Selfish) / Option Y (Altruistic): (独裁者の報酬, 受け手の報酬)
・Category 1 (Slope): ゲームの性質
  - Selfish: 利己的選択が可能
  - Receiver indiff.: 受け手無差別
  - Dictator indiff.: 独裁者無差別 
  - X Pareto/X Pareto-dom.: パレート支配
・Category 2 (Position): 独裁者の立場
  - Advantageous: 有利
  - Disadvantageous: 不利
  - Mixed: 混合
・Category 3 (Efficiency): 効率性
  - X/Y: どちらがより効率的か
  - None: 同程度
・Category 4 (Inequality): 不平等性
  - X/Y: どちらがより平等か
  - None: 同程度
  
# 3. 実験条件とディレクトリ構成

## 3.1 実験条件の分岐

※重要な変更点：すべての参加者がA役（独裁者）として実験に参加します。B役のデータは自動的に生成されます。

```
ai2025/
├── welcome_app/      # 共通：実験の説明・同意
├── real_effort_app/  # 共通：実労働タスク（1200ポイント獲得）
│
├── [条件1] AI学習条件
│   ├── AI_dictator_app/
│   │   ├── models.py          # ゲームのロジック
│   │   ├── pages.py          # ページの表示・遷移
│   │   └── payoff_scenarios.csv  # 配分シナリオ
│   └── ml_app/
│       ├── models.py          # AI学習・予測のロジック
│       └── pages.py          # 予測結果の表示
│
├── [条件2] 基本条件
│   └── Base_dictator_app/
│       ├── models.py          # ゲームのロジック
│       ├── pages.py          # ページの表示・遷移
│       └── payoff_scenarios.csv  # 配分シナリオ
│
└── questionnaire_app/  # 共通：最終アンケート

```

## 3.2 実験フローの分岐

### 条件1：AI学習条件

1. **共通パート**
   - `welcome_app`: 実験説明・同意確認
   - `real_effort_app`: 実労働タスク（1200ポイント獲得）

2. **ディクテーターゲーム（AI_dictator_app）**
   - 15回の通常ラウンド
     - A役が選択を行う（X or Y）
     - 選択データがAIの学習に使用される
   - AIによる予測（ml_app）
     - 15回分のデータでAIを学習
     - 新しいシナリオでAIが予測
     - AIの予測結果が16回目の配分
    - **データフロー概要**:
      1. **ディクテーターゲーム(1〜15回)**
        - 各回の選択結果（X/Y）と配分情報を保存
        - 保存内容: PayoffX_A, PayoffX_B, PayoffY_A, PayoffY_B, choice
      2. **学習データセット作成**
        - 15回分を(特徴量, ラベル)形式に変換
        - **特徴量**（全8項目）:
          1. A_x, B_x（X選択時の配分）
          2. A_y, B_y（Y選択時の配分）
          3. A_x + B_x, A_y + B_y（各選択肢の合計：効率性）
          4. A_x - A_y, B_x - B_y（各選択肢の差分：不平等度等）
        - **ラベル**: 選択肢（X=1, Y=0）
      3. **学習（Train）**
        - RandomForestClassifierで訓練
        - 設定: n_estimators=10, bootstrap=True, random_state固定
      4. **予測（Predict）**
        - 予測用6問からランダムに1問を選択（CSVファイルのIs_Training='False'のデータから）
        - 同じ特徴量形式で変換し予測
        - 多数決（各木のvote）で最終予測
      5. **報酬実装**
        - 予測結果（X/Y）に基づく配分を実行
        - 参加者の報酬として反映

3. **共通パート**
   - `questionnaire_app`: アンケート回答
   - 最終報酬の表示（実労働 + ランダム2回分）

### 条件2：基本条件

1. **共通パート**
   - `welcome_app`: 実験説明・同意確認
   - `real_effort_app`: 実労働タスク（1200ポイント獲得）

2. **ディクテーターゲーム（Base_dictator_app）**
   - 15回の通常ラウンド
     - A役が選択を行う（X or Y）
   - 16回目の追加ラウンド
     - 通常通りA役が選択
     - 予測用シナリオから1つ使用

3. **共通パート**
   - `questionnaire_app`: アンケート回答
   - 最終報酬の表示（実労働 + ランダム2回分）

## 2.3 条件による主な違い

1. **16回目のラウンド**
   - AI学習条件：AIが予測して決定
   - 基本条件：A役が通常通り決定

2. **使用アプリ**
   - AI学習条件：AI_dictator_app + ml_app
   - 基本条件：Base_dictator_app のみ

3. **説明内容**
   - AI学習条件：AIの学習・予測に関する説明あり
   - 基本条件：通常のディクテーターゲームの説明のみ

4. **報酬**
   - AI学習条件：15回目のラウンドからランダムに１回+AIが予測した16回目の報酬
   - 基本条件：16回目のラウンドからランダムに2回

---

## 3. 実施フローの詳細

1. **実験開始 (Welcome)**  
   - 被験者はoTreeリンクへアクセスし、ID入力 / 同意画面。
2. **実労働タスク (Part 1)**  
   - バイナリ文字列から特定のパターンを探す3問のタスクをこなし、全員が 1200pt を得る。  
   - 各タスク400ポイントで、`participant.vars['initial_endowment']`に合計を保存。
3. **ディクテーターゲーム (Part 2)**  
   - 2人1組に固定。role()で `A` または `B` に割り当て。  
   - 15回の通常ラウンドと1回の追加ラウンドを実施。
   - 全16ラウンドから「ランダム2回分」の抽選・ペイオフ計算を実施。
4. **AI予測 (AI学習条件のみ)**  
   - `ml_app` へ移行し、15回分の選択データを取得 → ランダムフォレスト学習 → 予測用6問からランダムに1問を選択 → 予測結果(X or Y)で配分を実行。
5. **アンケート (Part 3)**  
   - デモグラ情報入力、AI満足度や理解度アンケートなど。  
   - 終了画面で「最終獲得額」を表示。

---

## 4. データ保管と分析

- 実験終了後、oTree管理画面から全ラウンドのCSVをダウンロード可能。  
- `Player`単位で集計されるため、ラウンド数が16の場合は行が16倍になる。  
- AIの配分結果は `ml_app_Player` に格納。

---

## 5. 留意事項

1. **実労働タスク**  
   - バイナリ文字列から特定のパターンを探すタスクを3問実施。
   - 各タスク400ポイントで、合計1200ポイントを獲得。
   - 全員が同ポイントとなるよう設計。

2. **ディクテーターゲームのペイオフ設定**  
   - payoff_scenarios.csvに定義された配分パターンを使用。
   - Is_Training='True'のデータを学習用として使用。
   - Is_Training='False'のデータを予測用として使用。

3. **AIモデルの実装**  
   - ランダムフォレストを使用（scikit-learn）。
   - 15回のデータを全て学習に使用。
   - 予測用6問からランダムに1問を選択して予測を実施。

4. **報酬支払い**  
   - 実労働タスク報酬（1200ポイント）
   - 全16ラウンドから選ばれた2回分の報酬
   を合計して最終支払額を決定。