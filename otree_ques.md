# oTreeに関する質問事項

0. **元論文との整合性について**
   - 元論文では30回のゲームを実施していますが、16回（学習用）＋6回（予測用）に短縮した実装で問題ないでしょうか？
   - 特に以下の点について確認させてください：
     - 3つの円（不利な不平等、混合、有利な不平等）の分布バランス
     - Category 1-4の分布の偏り
     - 利害対立型（*印）のゲームの割合

2. **AIの予測について**
   - 処理の流れ：
     1. ディクテーターゲーム（3ラウンド）でのプレイヤーの選択を収集
        - 各ラウンドで保存するデータ：
          ```python
          {
              'round': ラウンド番号,
              'payoff_x_a': X選択時のA役の報酬,
              'payoff_x_b': X選択時のB役の報酬,
              'payoff_y_a': Y選択時のA役の報酬,
              'payoff_y_b': Y選択時のB役の報酬,
              'choice': プレイヤーの選択（'X'/'Y'）
          }
          ```

     2. 各ラウンドのデータを特徴量に変換
        - 入力データの形式：
          ```python
          {
              'Ax': X選択時のA役の報酬,
              'Bx': X選択時のB役の報酬,
              'Ay': Y選択時のA役の報酬,
              'By': Y選択時のB役の報酬,
              'choice': プレイヤーの選択
          }
          ```
        - 特徴量変換後：X_train shape=(3, 8), y_train shape=(3,)
          - 8次元の特徴量：[Ax, Bx, Ay, By] + [Ax/合計, Bx/合計, Ay/合計, By/合計]
          - ラベル：選択（X/Y）

     3. Random Forestモデルで学習
        - scikit-learnのRandomForestClassifierを使用
        - 学習データ：ラウンド数分の特徴量とラベル
        - ハイパーパラメータ：デフォルト値を使用

     4. 新しいシナリオで予測を実施
        - 予測用シナリオ：CSVファイルのIs_Training='False'のデータから選択
        - 同じ特徴量変換を適用して予測を実行
        - 出力：'X'または'Y'の予測結果

     5. 予測結果に基づいて報酬を計算
        - 予測が'X'の場合：scenario[0]の報酬を使用
        - 予測が'Y'の場合：scenario[1]の報酬を使用
        - A役とB役それぞれの報酬を`participant.vars['ai_prediction_payoff']`に保存

3. **報酬の計算について**
   - 学習用16ゲームのうち「ランダムに1回分」と「AIによる予測結果」の報酬の合算を報酬としていますがそれで適切でしょうか？